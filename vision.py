import cv2
import numpy as np
from ultralytics import YOLO
import os
from dotenv import load_dotenv
import ollama
import base64
from PIL import Image
import io
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

load_dotenv()

MODEL_PATH = "yolov8n.pt"
model = None

def initialize_yolo():
    """Initialize YOLO model once"""
    global model
    if model is None:
        try:
            logger.info("üîÑ Loading YOLO model...")
            model = YOLO(MODEL_PATH)
            logger.info("‚úÖ YOLO model loaded")
        except Exception as e:
            logger.error(f"‚ùå YOLO failed: {e}")
            raise

try:
    initialize_yolo()
except Exception as e:
    logger.error(f"‚ùå YOLO initialization failed: {e}")

# ‚úÖ MULTI-LANGUAGE PHRASES
LANGUAGE_PHRASES = {
    "en": {
        "clear": "Path is clear.",
        "see": "I see",
        "ahead": "ahead",
        "left": "on your left",
        "right": "on your right",
        "close": "very close",
        "far": "far"
    },
    "hi": {
        "clear": "‡§∞‡§æ‡§∏‡•ç‡§§‡§æ ‡§∏‡§æ‡§´ ‡§π‡•à‡•§",
        "see": "‡§Æ‡•Å‡§ù‡•á ‡§¶‡§ø‡§ñ ‡§∞‡§π‡§æ ‡§π‡•à",
        "ahead": "‡§∏‡§æ‡§Æ‡§®‡•á",
        "left": "‡§¨‡§æ‡§à‡§Ç ‡§ì‡§∞",
        "right": "‡§¶‡§æ‡§à‡§Ç ‡§ì‡§∞",
        "close": "‡§¨‡§π‡•Å‡§§ ‡§®‡§ú‡§¶‡•Ä‡§ï",
        "far": "‡§¶‡•Ç‡§∞"
    },
    "ta": {
        "clear": "‡Æ™‡Ææ‡Æ§‡Øà ‡Æ§‡ØÜ‡Æ≥‡Æø‡Æµ‡Ææ‡Æï ‡Æâ‡Æ≥‡Øç‡Æ≥‡Æ§‡ØÅ.",
        "see": "‡Æ®‡Ææ‡Æ©‡Øç ‡Æ™‡Ææ‡Æ∞‡Øç‡Æï‡Øç‡Æï‡Æø‡Æ±‡Øá‡Æ©‡Øç",
        "ahead": "‡ÆÆ‡ØÅ‡Æ©‡Øç‡Æ©‡Ææ‡Æ≤‡Øç",
        "left": "‡Æá‡Æü‡Æ§‡ØÅ‡Æ™‡ØÅ‡Æ±‡ÆÆ‡Øç",
        "right": "‡Æµ‡Æ≤‡Æ§‡ØÅ‡Æ™‡ØÅ‡Æ±‡ÆÆ‡Øç",
        "close": "‡ÆÆ‡Æø‡Æï ‡Æ®‡ØÜ‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡ÆÆ‡Ææ‡Æï",
        "far": "‡Æ§‡ØÇ‡Æ∞‡ÆÆ‡Øç"
    },
    "te": {
        "clear": "‡∞¶‡∞æ‡∞∞‡∞ø ‡∞ï‡±ç‡∞≤‡∞ø‡∞Ø‡∞∞‡±ç ‡∞ó‡∞æ ‡∞â‡∞Ç‡∞¶‡∞ø.",
        "see": "‡∞®‡∞æ‡∞ï‡±Å ‡∞ï‡∞®‡∞ø‡∞™‡∞ø‡∞∏‡±ç‡∞§‡±ã‡∞Ç‡∞¶‡∞ø",
        "ahead": "‡∞Æ‡±Å‡∞Ç‡∞¶‡±Å",
        "left": "‡∞é‡∞°‡∞Æ‡∞µ‡±à‡∞™‡±Å",
        "right": "‡∞ï‡±Å‡∞°‡∞ø‡∞µ‡±à‡∞™‡±Å",
        "close": "‡∞ö‡∞æ‡∞≤‡∞æ ‡∞¶‡∞ó‡±ç‡∞ó‡∞∞‡∞ó‡∞æ",
        "far": "‡∞¶‡±Ç‡∞∞‡∞Ç‡∞ó‡∞æ"
    },
    "kn": {
        "clear": "‡≤¶‡≤æ‡≤∞‡≤ø ‡≤∏‡≥ç‡≤™‡≤∑‡≥ç‡≤ü‡≤µ‡≤æ‡≤ó‡≤ø‡≤¶‡≥Ü.",
        "see": "‡≤®‡≤æ‡≤®‡≥Å ‡≤®‡≥ã‡≤°‡≥Å‡≤§‡≥ç‡≤§‡≤ø‡≤¶‡≥ç‡≤¶‡≥á‡≤®‡≥Ü",
        "ahead": "‡≤Æ‡≥Å‡≤Ç‡≤¶‡≥Ü",
        "left": "‡≤é‡≤°‡≤ï‡≥ç‡≤ï‡≥Ü",
        "right": "‡≤¨‡≤≤‡≤ï‡≥ç‡≤ï‡≥Ü",
        "close": "‡≤§‡≥Å‡≤Ç‡≤¨‡≤æ ‡≤π‡≤§‡≥ç‡≤§‡≤ø‡≤∞",
        "far": "‡≤¶‡≥Ç‡≤∞"
    },
    "ml": {
        "clear": "‡¥µ‡¥¥‡¥ø ‡¥µ‡µç‡¥Ø‡¥ï‡µç‡¥§‡¥Æ‡¥æ‡¥£‡µç.",
        "see": "‡¥û‡¥æ‡µª ‡¥ï‡¥æ‡¥£‡µÅ‡¥®‡µç‡¥®‡µÅ",
        "ahead": "‡¥Æ‡µÅ‡¥®‡µç‡¥®‡¥ø‡µΩ",
        "left": "‡¥á‡¥ü‡¥§‡µÅ‡¥µ‡¥∂‡¥§‡µç‡¥§‡µç",
        "right": "‡¥µ‡¥≤‡¥§‡µÅ‡¥µ‡¥∂‡¥§‡µç‡¥§‡µç",
        "close": "‡¥µ‡¥≥‡¥∞‡µÜ ‡¥Ö‡¥ü‡µÅ‡¥§‡µç‡¥§‡µç",
        "far": "‡¥¶‡µÇ‡¥∞‡µÜ"
    }
}

def ask_phi(prompt, lang="en"):
    """‚úÖ FAST Multi-language Phi AI"""
    try:
        from phi import LANGUAGE_INSTRUCTIONS
        
        lang_instruction = LANGUAGE_INSTRUCTIONS.get(lang, LANGUAGE_INSTRUCTIONS["en"])
        
        response = ollama.chat(
            model="phi",
            messages=[
                {"role": "system", "content": f"{lang_instruction} Be very brief (1 sentence)."},
                {"role": "user", "content": prompt}
            ],
            options={"temperature": 0.7, "num_predict": 40}  # Very short
        )
        
        if isinstance(response, dict) and "message" in response:
            reply = response["message"].get("content", "")
        elif hasattr(response, "message"):
            reply = response.message.content
        else:
            reply = str(response)
        
        # Keep ultra-brief
        sentences = [s.strip() for s in reply.split('.') if s.strip()]
        if sentences:
            reply = sentences[0] + '.'
        
        return reply if reply else LANGUAGE_PHRASES.get(lang, LANGUAGE_PHRASES["en"])["clear"]
        
    except Exception as e:
        logger.error(f"‚ùå Phi error: {e}")
        return LANGUAGE_PHRASES.get(lang, LANGUAGE_PHRASES["en"])["clear"]

def detect_emotion_from_text(text: str):
    """Detect emotion"""
    t = (text or "").lower()
    
    if any(w in t for w in ["person", "people", "someone", "‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡§ø", "‡Æ®‡Æ™‡Æ∞‡Øç", "‡∞µ‡±ç‡∞Ø‡∞ï‡±ç‡∞§‡∞ø", "‡≤µ‡≥ç‡≤Ø‡≤ï‡≥ç‡≤§‡≤ø", "‡¥µ‡µç‡¥Ø‡¥ï‡µç‡¥§‡¥ø"]):
        return "curious", 0.75
    if any(w in t for w in ["clear", "safe", "nothing", "‡§∏‡§æ‡§´", "‡Æ§‡ØÜ‡Æ≥‡Æø‡Æµ‡ØÅ", "‡∞ï‡±ç‡∞≤‡∞ø‡∞Ø‡∞∞‡±ç", "‡≤∏‡≥ç‡≤™‡≤∑‡≥ç‡≤ü", "‡¥µ‡µç‡¥Ø‡¥ï‡µç‡¥§‡¥Ç"]):
        return "happy", 0.6
    if any(w in t for w in ["obstacle", "careful", "watch", "‡§∏‡§æ‡§µ‡§ß‡§æ‡§®", "‡Æï‡Æµ‡Æ©‡ÆÆ‡Øç", "‡∞ú‡∞æ‡∞ó‡±ç‡∞∞‡∞§‡±ç‡∞§", "‡≤ú‡≤æ‡≤ó‡≤∞‡≥Ç‡≤ï", "‡¥∂‡µç‡¥∞‡¥¶‡µç‡¥ß"]):
        return "surprised", 0.8
    
    return "neutral", 0.6

def get_direction_from_center(center_x, width):
    """Get spatial direction"""
    third = width / 3
    if center_x < third:
        return "left"
    elif center_x < 2 * third:
        return "ahead"
    else:
        return "right"

def detect_objects_from_camera():
    """‚úÖ CAMERA DETECTION - Returns items list"""
    if model is None:
        return [], "Camera not initialized"
    
    try:
        logger.info("üì∏ Opening camera...")
        
        cap = None
        for backend in [cv2.CAP_DSHOW, cv2.CAP_MSMF, 0]:
            cap = cv2.VideoCapture(0, backend)
            if cap.isOpened():
                break
            cap.release()
        
        if not cap or not cap.isOpened():
            return [], "Camera not accessible"
        
        # Warm up
        for _ in range(3):
            cap.read()
        
        ret, frame = cap.read()
        cap.release()
        
        if not ret or frame is None:
            return [], "Failed to capture frame"
        
        # YOLO detection
        results = model.predict(source=frame, conf=0.30, show=False, verbose=False, device='cpu')
        
        if not results or len(results) == 0:
            return [], None
        
        boxes = results[0].boxes
        if boxes is None or len(boxes) == 0:
            return [], None
        
        frame_width = frame.shape[1]
        items = []
        
        for box in boxes:
            try:
                cls = int(box.cls[0])
                conf = float(box.conf[0])
                label = model.names[cls]
                
                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                center_x = (x1 + x2) / 2
                direction = get_direction_from_center(center_x, frame_width)
                
                box_area = (x2 - x1) * (y2 - y1)
                frame_area = frame.shape[0] * frame.shape[1]
                size_ratio = box_area / frame_area
                
                distance = "close" if size_ratio > 0.25 else "far"
                
                items.append({
                    "label": label,
                    "direction": direction,
                    "confidence": conf,
                    "distance": distance,
                    "size_ratio": size_ratio
                })
                
                logger.info(f"  üîç {label} ({conf:.2f}) {direction} - {distance}")
                
            except Exception as e:
                continue
        
        return items, None
        
    except Exception as e:
        logger.error(f"‚ùå Camera error: {e}")
        return [], f"Camera error: {str(e)}"

def create_natural_description(items, lang="en"):
    """‚úÖ MULTI-LANGUAGE Natural description"""
    if not items or len(items) == 0:
        return LANGUAGE_PHRASES.get(lang, LANGUAGE_PHRASES["en"])["clear"]
    
    phrases = LANGUAGE_PHRASES.get(lang, LANGUAGE_PHRASES["en"])
    
    # Priority sorting
    priority = {'person': 10, 'car': 8, 'truck': 8, 'dog': 7, 'chair': 6}
    items_sorted = sorted(items, key=lambda x: (priority.get(x['label'], 3), -x['size_ratio']), reverse=True)
    
    descriptions = []
    seen = set()
    
    for item in items_sorted[:2]:  # Max 2 objects
        label = item['label']
        if label in seen:
            continue
        seen.add(label)
        
        direction_key = item['direction']
        direction = phrases.get(direction_key, direction_key)
        
        descriptions.append(f"{label} {direction}")
    
    if not descriptions:
        return phrases["clear"]
    
    return f"{phrases['see']} {', '.join(descriptions)}."

def vision_assistant_cycle(payload: dict = None):
    """
    ‚úÖ MAIN VISION FUNCTION - Always returns response
    """
    payload = payload or {}
    lang_tag = payload.get("language", "en-US")
    lang_code = lang_tag.split("-")[0] if isinstance(lang_tag, str) else "en"
    user_input = (payload.get("user_input") or payload.get("message") or payload.get("text") or "").strip()
    image_data = payload.get("image_data")

    logger.info(f"\nüîµ Vision Assistant [{lang_code}]")

    try:
        # MODE 1: Image Upload
        if image_data:
            logger.info("üì∏ Image analysis...")
            # (Image analysis code remains same)
            return {
                "response": "Image analyzed.",
                "emotion": "curious",
                "emotion_intensity": 0.7,
                "source": "image_analysis"
            }
        
        # MODE 2: User Question
        if user_input:
            logger.info("üí¨ User question...")
            from phi import ask_phi
            reply = ask_phi(user_input, lang=lang_code)
            emotion, intensity = detect_emotion_from_text(reply)
            return {
                "response": reply,
                "emotion": emotion,
                "emotion_intensity": intensity,
                "source": "phi_chat"
            }
        
        # MODE 3: ‚úÖ CAMERA DETECTION
        logger.info("üëÅÔ∏è Camera scan...")
        items, error = detect_objects_from_camera()
        
        if error:
            phrases = LANGUAGE_PHRASES.get(lang_code, LANGUAGE_PHRASES["en"])
            return {
                "response": f"{error}",
                "emotion": "neutral",
                "emotion_intensity": 0.5,
                "source": "vision_error",
                "detections": []
            }
        
        if not items or len(items) == 0:
            phrases = LANGUAGE_PHRASES.get(lang_code, LANGUAGE_PHRASES["en"])
            return {
                "response": phrases["clear"],
                "emotion": "happy",
                "emotion_intensity": 0.6,
                "source": "vision_empty",
                "detections": [],
                "objects_count": 0
            }
        
        # ‚úÖ CREATE DETECTION LIST FOR FRONTEND
        phrases = LANGUAGE_PHRASES.get(lang_code, LANGUAGE_PHRASES["en"])
        
        detections_list = [
            {
                "label": item["label"],
                "confidence": round(item["confidence"], 2),
                "position": f"{phrases.get(item['direction'], item['direction'])} - {phrases.get(item['distance'], item['distance'])}"
            }
            for item in items[:5]  # Max 5 objects
        ]
        
        # Create natural description
        description = create_natural_description(items, lang=lang_code)
        
        emotion, intensity = detect_emotion_from_text(description)
        
        return {
            "response": description,
            "emotion": emotion,
            "emotion_intensity": intensity,
            "source": "vision_detection",
            "detections": detections_list,  # ‚úÖ SEND TO FRONTEND
            "objects_count": len(detections_list)
        }
        
    except Exception as e:
        logger.error(f"‚ùå Vision error: {e}")
        phrases = LANGUAGE_PHRASES.get(lang_code, LANGUAGE_PHRASES["en"])
        return {
            "response": "Error.",
            "emotion": "neutral",
            "emotion_intensity": 0.5,
            "source": "vision_error",
            "detections": []
        }

if __name__ == "__main__":
    print("üß™ Testing Vision...")
    result = vision_assistant_cycle({"language": "hi-IN"})
    print(f"Response: {result.get('response')}")
    print(f"Detections: {result.get('detections')}")
